# HumaniBench: A Human-Centric Benchmark for Large Multimodal Models Evaluation

<p align="center">
  <img src="https://github.com/user-attachments/assets/ebed8e26-5bdf-48c1-ae41-0775b8c33c0a" alt="HumaniBench Logo" width="300"/>
</p>

<p align="center">
  <b>Dataset:</b> <a href="https://huggingface.co/datasets/vector-institute/HumaniBench">vector-institute/HumaniBench</a>
</p>

---

## ğŸ§  Overview

As multimodal generative AI systems become increasingly integrated into human-centered applications, evaluating their **alignment with human values** has become critical.

**HumaniBench** is the **first comprehensive benchmark** designed to evaluate **Large Multimodal Models (LMMs)** on **seven Human-Centered AI (HCAI) principles**:

* **Fairness**
* **Ethics**
* **Understanding**
* **Reasoning**
* **Language Inclusivity**
* **Empathy**
* **Robustness**

This repository provides code and scripts for evaluating LMMs across **7 human-aligned tasks**.

---

## ğŸ“¦ Features

* ğŸ“· **32,000+ Real-World Imageâ€“Question Pairs**
* âœ… **Human-Verified Ground Truth Annotations**
* ğŸŒ **Multilingual QA Support (10+ languages)**
* ğŸ§  **Open and Closed-Ended VQA Formats**
* ğŸ§ª **Visual Robustness & Bias Stress Testing**
* ğŸ“‘ **Chain-of-Thought Reasoning + Perceptual Grounding**


---

## ğŸ“‚ Evaluation Tasks Overview

| Task                               | Focus                                                                                          | Folder                             |
| :--------------------------------- | :--------------------------------------------------------------------------------------------- | :--------------------------------- |
| **Task 1: Scene Understanding**    | Visual reasoning + bias/toxicity analysis in social attributes (gender, age, occupation, etc.) | `code/task1_Scene_Understanding`   |
| **Task 2: Instance Identity**      | Visual reasoning in culturally rich, socially grounded settings                                | `code/task2_Instance_Identity`     |
| **Task 3: Multiple Choice QA**     | Structured attribute recognition via multi-choice questions                                    | `code/task3_Multiple_Choice_VQA`   |
| **Task 4: Multilingual Visual QA** | VQA across 10+ languages, including low-resource ones                                          | `code/task4_Multilingual`          |
| **Task 5: Visual Grounding**       | Bounding box localization of socially salient regions                                          | `code/task5_Visual_Grounding`      |
| **Task 6: Empathetic Captioning**  | Human-style emotional captioning evaluation                                                    | `code/task6_Empathetic_Captioning` |
| **Task 7: Image Resilience**       | Robustness testing via image perturbations                                                     | `code/task7_Image_Resilience`      |

> ğŸ” Each task folder includes a README with setup instructions, task structure, and metrics.

---

## ğŸ§¬ Pipeline

**Three-stage process:**

1. **Data Collection**
   Curated from global news imagery, tagged by social attributes (age, gender, race, occupation, sport)

2. **Annotation**
   GPT-4oâ€“assisted labeling + human expert verification

3. **Evaluation**
   Comprehensive scoring across:

   * Accuracy
   * Fairness
   * Robustness
   * Empathy
   * Faithfulness

---

## ğŸ”‘ Key Insights

* ğŸ” **Bias persists**, especially across gender and race
* ğŸŒ **Multilingual gaps** affect low-resource language performance
* â¤ï¸ **Empathy and ethics** vary significantly by model family
* ğŸ§  **Chain-of-Thought reasoning** improves performance but doesnâ€™t fully mitigate bias
* ğŸ§ª **Robustness tests** reveal fragility to noise, occlusion, and blur

---

## ğŸ“š Citation

If you use HumaniBench or this evaluation suite in your work, please cite:

```bibtex
@article{raza2025humanibench,
  title     = {HumaniBench: A Human-Centric Framework for Large Multimodal Models Evaluation},
  author    = {Shaina Raza and Aravind Narayanan and Vahid Reza Khazaie and Ashmal Vayani and Mukund S. Chettiar and Amandeep Singh and Mubarak Shah and Deval Pandya},
  year      = {2025},
  institution = {Vector Institute},
  note      = {Under review}
}

```

---

## ğŸ“¬ Contact

For questions, collaborations, or dataset access requests, please [open an issue](https://github.com/VectorInstitute/HumaniBench/issues) in this repository or contact the corresponding author at [shaina.raza@vectorinstitute.ai](mailto:shaina.raza@vectorinstitute.ai), as listed in the paper.

---

### âš¡ HumaniBench promotes trustworthy, fair, and human-centered multimodal AI.

**We invite researchers, developers, and policymakers to explore, evaluate, and extend HumaniBench. ğŸš€**

---

