<!DOCTYPE html>
<html>
<script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
<script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>

<meta property='og:title' content='HumaniBench: A Human-Centric Benchmark for Large Multimodal Models Evaluation' />
<meta property='og:image' content='' />
<meta property='og:description' content='' />
<meta property='og:url' content='https://github.com/' />
<meta property='og:image:width' content='1200' />
<meta property='og:image:height' content='663' />
<!-- TYPE BELOW IS PROBABLY: 'website' or 'article' or look on https://ogp.me/#types -->
<meta property="og:type" content='website' />

<head>
  <!-- Google tag (gtag.js) -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=G-9VZKE74FPW"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag() { dataLayer.push(arguments); }
    gtag('js', new Date());

    gtag('config', 'G-9VZKE74FPW');
  </script>
  <meta charset="utf-8">
  <meta name="description" content="HumaniBench: A Benchmark for Human-Centric Alignment in Multimodal Large Language Models">
  <meta name="keywords" content="
    HumaniBench, Multimodal Large Language Models, Human-Centric Alignment, Benchmark, Dataset, Evaluation, Fairness, Ethics, Perceptual Honesty, Multilingual Equity, Empathy">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <title>HumaniBench: A Human-Centric Benchmark for Large Multimodal Models Evaluation</title>

  <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro" rel="stylesheet">
  <link rel="stylesheet" href="./static/css/bulma.min.css">
  <link rel="stylesheet" href="./static/css/bulma-carousel.min.css">
  <link rel="stylesheet" href="./static/css/bulma-slider.min.css">
  <link rel="stylesheet" href="./static/css/tab_gallery.css">
  <link rel="stylesheet" href="./static/css/fontawesome.all.min.css">
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
  <link rel="stylesheet" href="./static/css/index.css">
  <link rel="icon" href="./static/images/HumaniBenchLogo.ico">
  <link rel="stylesheet" href="juxtapose/css/juxtapose.css">

  <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
  <script defer src="./static/js/fontawesome.all.min.js"></script>
  <script src="./static/js/bulma-carousel.min.js"></script>
  <script src="./static/js/bulma-slider.min.js"></script>
  <script src="./static/js/index.js"></script>
  <script src="./static/js/magnifier.js"></script>
  <link href="https://fonts.cdnfonts.com/css/menlo" rel="stylesheet">
  <link rel="stylesheet" href="./static/css/image_card_fader.css">
  <link rel="stylesheet" href="./static/css/image_card_slider.css">

</head>

<style>
  @import url('https://fonts.cdnfonts.com/css/menlo');
  .hero.hero-top {
    background: linear-gradient(135deg, #f0f4f8, #d9e2ec);
  }
  .hero.hero-top {
    color: #222; /* or white if background is dark */
  }


</style>


<body>
  <!-- <section class="hero"> -->
    <section class="hero hero-top">

    
    <div class="hero-body">
      <div class="container is-max-desktop">
        <div class="columns is-centered">
          <div class="column has-text-centered">
            <!-- <h1 class="title is-2 publication-title">VLDBench: <u>V</u>ision <u>L</u>anguage Models -->
              <!-- <u>D</u>isinformation Detection <u>Bench</u>mark -->
            <img src="./static/images/HumaniBenchLogo.ico" alt="HumaniBench Logo" style="max-height: 150px; margin-bottom: 10px;">
            <h1 class="title is-2 publication-title">
              <span style="font-weight: 900;">HumaniBench</span>: A Human-Centric Benchmark for Large Multimodal Models Evaluation
            </h1>
            
            
            <div class="is-size-5 publication-authors">
              <span class="author-block"><a href="https://scholar.google.com/citations?user=chcz7RMAAAAJ&hl=en">Shaina Raza</a><sup>1</sup>,</span>
              <span class="author-block"><a href="https://aravind-3105.github.io/">Aravind Narayanan</a><sup>1*</sup>,</span>
              <span class="author-block"><a href="https://scholar.google.com/citations?user=lEWvRbIAAAAJ&hl=en">Vahid
                  Reza Khazaie</a><sup>1*</sup>,</span>
              <span class="author-block"><a href="https://ashmalvayani.github.io/">Ashmal Vayani</a><sup>2*</sup>,</span>
              <span class="author-block"><a
                  href="https://scholar.google.com/citations?hl=en&user=Gn1FIg4AAAAJ">Mukund Sayeeganesh Chettiar</a><sup>1</sup>,</span>
              <span class="author-block"><a href="#">Amandeep Singh</a><sup>1</sup>,</span>
                            <span class="author-block"><a href="https://scholar.google.com/citations?user=p8gsO3gAAAAJ&hl=en">Mubarak Shah</a><sup>2</sup>,</span>

              <span class="author-block"><a href="https://scholar.google.com/citations?user=o1XZyLMAAAAJ&hl=en">Deval Pandya</a><sup>1</sup></span>
            </div>
            <div class="is-size-5 publication-authors">
              <span class="author-block"><sup>1</sup>Vector Institute for Artificial Intelligence</span>
              <span class="author-block"><sup>2</sup>University of Central Florida</span>
            </div>


            <div class="column has-text-centered">
              <div class="publication-links">
                <!-- PDF Link. -->
                <span class="link-block">
                  <a href="https://arxiv.org/abs/2505.11454" target="_blank" class="external-link button is-normal is-rounded is-dark">
                    <span class="icon">
                      <i class="ai ai-arxiv"></i>
                    </span>
                    <span>arXiv</span>
                  </a>
                </span>
                </span>
                <span class="link-block">
                  <a href="https://huggingface.co/datasets/vector-institute/HumaniBench" target="_blank"
                    class="external-link button is-normal is-rounded is-dark">
                    <span class="icon">
                      <i class="fas fa-database"></i>
                    </span>
                    <span>Dataset</span>
                  </a>
                </span>
                <span class="link-block">
                  <a href="https://github.com/VectorInstitute/HumaniBench" target="_blank"
                    class="external-link button is-normal is-rounded is-dark">
                    <span class="icon">
                      <i class="fab fa-github"></i>
                    </span>
                    <span>Code</span>
                  </a>
                </span>
            
                <span class="link-block">
                  <a href="#bibtex" class="external-link button is-normal is-rounded is-dark">
                    <span class="icon">
                      <i class="ai ai-obp"></i>
                    </span>
                    <span>BibTex</span>
                  </a>
                </span>
              </div>
            </div>
          </div>
        </div>
      </div>
    </div>
  </section>

  <section class="hero teaser">
    <div class="container is-max-desktop">
      <div class="hero-body">
        <p align="justify"><br>
          <strong>HumaniBench</strong> is the first comprehensive benchmark designed to evaluate <em>large multimodal models (LMMs)</em> on key human-centered principles such as <strong>fairness</strong>, <strong>ethics</strong>, <strong>understanding</strong>, <strong>reasoning</strong>, <strong>language</strong>, <strong>inclusivity</strong>, <strong>empathy</strong> and <strong>robustness</strong>. Built on <strong>32,000 real-world image–question pairs</strong> across seven diverse tasks, HumaniBench goes beyond standard accuracy metrics to probe how well models align with human needs, values, and expectations. By providing a rigorous, expert-verified evaluation framework, HumaniBench aims to guide the development of LMMs that are not only <em>powerful</em> and <em>versatile</em> but also <strong>ethical</strong>, <strong>inclusive</strong>, and <strong>trustworthy</strong> in real-world applications.</p>
        <!-- <br> -->

        <div class="column">
          <div style="text-align:center;">
            <img src="static/images/HumaniBenchTeaser.jpg" style="max-width:100%">
            <div class="content has-text-justified">
              <p align="justify"> <b> <span>Figure</span></b>:
                This figure shows AI-assisted annotation of the HumaniBench dataset followed by domain-expert verification (top). The benchmark  has both open-ended and closed VQA setting and includes seven multimodal tasks (T1–T7). There are seven human-centric principle defined and each task is associated with one or more human-aligned principles (center). Task examples include VQA across modalities and languages. The bottom panel presents the evaluation scheme comprising LLM-based judgments, and standard metrics as needed by the specific task.
              </p>
            </div>
          </div>
        </div>

        <br><br>
      </div>
    </div>

  </section>

  <section class="hero is-light is-small">
    <div class="hero-body">
      <div class="container is-max-desktop has-text-centered">
        <!-- Visual Effects. -->
        <h2 class="title is-3">Abstract</h2>
        <div class="content has-text-justified">
          <p>
            Large Multimodal Models (LMMs) perform exceptional at vision-language tasks but still face challenges in human-centered criteria such as fairness, ethics, empathy,  and inclusivity, principles essential for true alignment with human values. We introduce <strong>HumaniBench</strong>, a benchmark comprising approximately 32 K image–question pairs from real-world imagery, annotated via a scalable GPT-4o-assisted pipeline and rigorously verified by domain experts. HumaniBench evaluates models across seven human-aligned principles—<i>fairness, ethics, understanding, reasoning, language inclusivity, empathy, and robustness</i>—covering diverse tasks that include both open- and closed-ended visual question answering (VQA). Benchmarking 15 state-of-the-art LMMs (open- and closed-source) reveals that proprietary models generally lead; however, significant gaps remain in robustness and visual grounding, while open-source models struggle to balance accuracy with adherence to human-aligned principles such as ethics and inclusivity. HumaniBench is the first unified, real-world benchmark explicitly designed around Human-Centered AI principles, providing a rigorous testbed to diagnose alignment issues and guide models toward behavior that is accurate, ethical, inclusive, and socially responsible. To promote transparency and foster future research, we publicly release the full dataset and evaluation code.<br>
          </p>
        </div>
      </div>
    </div>


  </section>




  <section class="section">
    <div class="container is-max-desktop">
      <!-- Abstract. -->
      <div class="columns is-centered has-text-centered">
        <div class="column is-four-fifths">
          <h2 class="title is-3"><em>HumaniBench</em> is the first evaluation framework to unify these human-centric principles—such as fairness, ethics, empathy, and multilingual understanding—into a single suite.</h2>


          <div class="content has-text-justified">
            <p>
            <h5> <b> Main contributions: </b></h5>
            <ol>
              <li>We release a corpus of about 32K image–text pairs curated from real-world news articles on diverse, socially relevant topics. For each image we generate a caption and assign a social-attribute tag (age, gender, race, sport, or occupation) to create rich metadata for downstream task annotations</li>
  
              <li>Guided by HCAI, we distill seven human-aligned principles into seven realistic LMM tasks: <i>(T1) Scene Understanding, (T2) Instance Identity, (T3) Multiple-Choice VQA, (T4) Multilinguality, (T5) Visual Grounding, (T6) Empathetic Captioning, and (T7) Image Resilience</i>. Each sample in each task is labeled through a semi-automated GPT-4o workflow and rigorously verified by domain experts to ensure reliable ground truth at scale</li>
              
              <li>We benchmark 15 leading LMMs—13 open-source and 2 proprietary—delivering the first holistic measure of their human-readiness. All data, code, and evaluation scripts are publicly released to foster transparent and reproducible research</li>
            </ol>
            </p>
          </div>
        </div>
      </div>
      
      <!--/ Abstract. -->

    </div>
  </section>

  <section class="section">

    <!--/ Matting. -->
    <div class="container is-max-desktop">

      <!-- Latent space editing applications -->
      <div class="columns is-centered">
        <div class="column is-full-width">
          
          <h2 class="title is-3 has-text-centered">HumaniBench Framework Overview</h2>

          <div class="content has-text-justified">
            <!-- <p>VLDBench is a comprehensive classification multimodal benchmark for disinformation detection in news articles. We categorized our data into 13 unique news categories by providing image-text pairs to GPT-4o.</p> -->
            <div class="content has-text-centered"><img src="static/images/Table1.jpg"  style="max-width:80%">
              <p class="content has-text-justified"> <b> <span>Table</span></b>: Comparison of LMM benchmarks with our seven human-centric principles. Columns are marked ✓ if covered, ✗ if not, or ∼ if partially covered. “HC” denotes human-centric coverage; “Data Source” indicates whether images are real (R) or synthetic (S), with (SD) for Stable Diffusion.</p>
            </div>
          </div>


        </div>
      </div>

      <!--/ Matting. -->
      <div class="container is-max-desktop">

        <!-- Latent space editing applications -->
        <div class="columns is-centered">
          <div class="column is-full-width">
          <h2 class="title is-3 has-text-centered">Annotation Pipeline</h2>
            <div class="content has-text-centered">
              <img src="static/images/HumaniBenchAnnotation.jpg" style="max-width:80%">
              <p class="content has-text-justified"><b> <span>Figure</span></b>: Dataset creation pipeline: images are extracted, filtered for duplicates using CLIP, captions & social attributes by GPT-4o, verified by humans, resulting in 13K unique images.
              </p>
            </div>


            <div class="content has-text-justified">
              <p align="justify"> 
                <!-- link datasheet.pdf  -->
                The datasheet for the final HumaniBench dataset can be found <a href="../datasheet.pdf" target="_blank">here</a>.

              </p>
          </div>


          <h2 class="title is-3 has-text-centered">Tasks Overview</h2>
            <!-- <div class="content has-text-centered">
              <img src="static/images/Table2.png" style="max-width:80%">
              <p class="content has-text-justified"><b> <span>Figure</span></b>: Overview of HumaniBench Tasks Grouped by Alignment Dimension.</p>
            </div>-->

            <div class="content has-text-centered">
              <img src="static/images/HumaniBenchOverview.jpg" style="max-width:80%">
              <p class="content has-text-justified"><b> <span>Figure</span></b>: <strong>HumaniBench</strong> overview. ♠ = covers <em>all</em> seven principles; all tasks are evaluated across five social attributes (age, gender, race, occupation, sport). Sections: (i) icon row, (ii) principle definitions, (iii) seven-task suite (I = image, T = text, B = bounding box), and (iv) metric–principle alignment.</p><br>
            </div>
            

            <div class="content has-text-justified">
              <p align="justify"> 
                Each of the seven tasks in HumaniBench corresponds to one or more of the seven core human-centric principles that we defined and is designed to reflect realistic, complex, and diverse scenarios.
              </p>
              <ul>
                <li><strong>T1: Scene Understanding</strong><br>
                  Evaluates models on open-ended reasoning over everyday scenes with socially grounded attributes using both standard and chain-of-thought prompts.
                </li>
              
                <li><strong>T2: Instance Identity</strong><br>
                  Tests the model's ability to identify and describe key individuals or objects in an image based on identity-relevant features.
                </li>
              
                <li><strong>T3: Multiple-Choice VQA</strong><br>
                  Assesses fine-grained visual recognition through multiple-choice questions focused on socially salient visual attributes.
                </li>
              
                <li><strong>T4: Multilinguality</strong><br>
                  Measures fairness and consistency in visual question answering across ten languages representing diverse cultural and linguistic contexts.
                </li>
              
                <li><strong>T5: Visual Grounding</strong><br>
                  Evaluates how accurately a model links textual references to visual regions using bounding boxes.
                </li>
              
                <li><strong>T6: Empathetic Captioning</strong><br>
                  Tests the model’s ability to generate emotionally sensitive yet factual image captions for complex social scenes.
                </li>
              
                <li><strong>T7: Image Resilience</strong><br>
                  Assesses robustness by comparing model responses to original and visually perturbed versions of the same image.
                </li>
              </ul>
              
          </div>

        </div>
      </div>
      <br><br>
      <div class="content has-text-centered">
        <h3 class="title is-4 has-text-justified">Large Multimodal Systems under Evaluation</h3>
        <div class="content has-text-justified">
          <p align="justify"> For HumaniBench, we evaluate thirteen state-of-the-art open-source Large Multimodal Models (LMMs) across diverse human-alignment tasks. These models represent a range of vision encoders, language backbones, and fusion techniques. By focusing exclusively on vision-language models, HumaniBench provides a rigorous testbed for assessing human-centered reasoning, fairness, empathy, and robustness in real-world multimodal scenarios. </p>
          <!-- I want width to be 80% center aligned-->
          <table style="width:60%; margin: 0 auto; border-collapse: collapse; font-family: sans-serif;">
            <thead style="background-color: #f2f2f2;">
              <tr>
                <th style="padding: 8px; border-bottom: 2px solid #ccc; font-size: 1.1em; text-align: left;">
                  Large Multimodal Models (LMMs)
                </th>
              </tr>
            </thead>
            <tbody>
              <tr><td style="padding: 6px; border-bottom: 1px solid #e0e0e0;"><a href="https://huggingface.co/THUDM/cogvlm2-llama3-chat-19B" target="_blank">CogVLM2-19B</a></td></tr>
              <tr><td style="padding: 6px; border-bottom: 1px solid #e0e0e0;"><a href="https://huggingface.co/CohereLabs/aya-vision-8b" target="_blank">Cohere Aya Vision 8B</a></td></tr>
              <tr><td style="padding: 6px; border-bottom: 1px solid #e0e0e0;"><a href="https://huggingface.co/deepseek-ai/deepseek-vl2-small" target="_blank">DeepSeek VL2 Small</a></td></tr>
              <tr><td style="padding: 6px; border-bottom: 1px solid #e0e0e0;"><a href="https://huggingface.co/THUDM/glm-4v-9b" target="_blank">GLM-4V-9B</a></td></tr>
              <tr><td style="padding: 6px; border-bottom: 1px solid #e0e0e0;"><a href="https://huggingface.co/OpenGVLab/InternVL2_5-8B" target="_blank">InternVL2.5 8B</a></td></tr>
              <tr><td style="padding: 6px; border-bottom: 1px solid #e0e0e0;"><a href="https://huggingface.co/deepseek-ai/Janus-Pro-7B" target="_blank">Janus-Pro-7B</a></td></tr>
              <tr><td style="padding: 6px; border-bottom: 1px solid #e0e0e0;"><a href="https://huggingface.co/meta-llama/Llama-3.2-11B-Vision-Instruct" target="_blank">LLaMA 3.2 11B Vision Instruct</a></td></tr>
              <tr><td style="padding: 6px; border-bottom: 1px solid #e0e0e0;"><a href="https://huggingface.co/llava-hf/llava-v1.6-vicuna-7b-hf" target="_blank">LLaVA-v1.6-Vicuna-7B</a></td></tr>
              <tr><td style="padding: 6px; border-bottom: 1px solid #e0e0e0;"><a href="https://huggingface.co/allenai/Molmo-7B-D-0924" target="_blank">Molmo-7B</a></td></tr>
              <tr><td style="padding: 6px; border-bottom: 1px solid #e0e0e0;"><a href="https://huggingface.co/microsoft/phi-4" target="_blank">Phi 4</a></td></tr>
              <tr><td style="padding: 6px; border-bottom: 1px solid #e0e0e0;"><a href="https://huggingface.co/microsoft/Phi-3.5-vision-instruct" target="_blank">Phi 3.5 Vision Instruct</a></td></tr>
              <tr><td style="padding: 6px; border-bottom: 1px solid #e0e0e0;"><a href="https://huggingface.co/Qwen/Qwen2.5-VL-7B-Instruct" target="_blank">Qwen2.5-VL-7B Instruct</a></td></tr>
              <tr><td style="padding: 6px;"><a href="#" target="_blank">Gemma 3</a></td></tr>
          
              <!-- Closed Source Models -->
              <tr><td style="padding: 6px; border-bottom: 1px solid #e0e0e0;">
                <a href="https://platform.openai.com/docs/models/gpt-4o" target="_blank">OpenAI GPT-4o</a> 
                <span style="font-size: 0.9em; color: gray;">(Closed source)</span>
              </td></tr>
              <tr><td style="padding: 6px;">
                <a href="https://ai.google.dev/gemini-api/docs/models" target="_blank">Gemini 2.0</a> 
                <span style="font-size: 0.9em; color: gray;">(Closed source)</span>
              </td></tr>
            </tbody>
          </table>
          
          
        
        
      </div>

      <br><br>

      <div class="columns is-centered">
        <div class="column is-full-width has-text-centered">
          <h2 class="title is-3">General Performance Overview on HumaniBench</h2>

          <div class="content has-text-justified">
            <p> We evaluated both open‐source and closed‐source MLLMs on HumaniBench using a variety of tasks (T1–T7). This section presents our main empirical findings and highlights key challenges for MLLMs. </p>
          </div>

          <h3 class="title is-4 has-text-justified">Performance Across Human-Aligned Principles</h3>

          <div class="content has-text-justified">
            <p>Closed-source large multimodal models generally achieve the highest overall performance, showing strengths in fairness, reasoning, safety, multilingual coverage, and empathetic responses. They tend to produce more balanced outputs across demographics and benefit from stronger safety alignment and reinforcement learning techniques.</p>
            <p>Open-source models, however, excel in specific capabilities. Some outperform closed alternatives in understanding tasks, particularly object recognition and visual grounding, and achieve higher robustness through specialized stabilization strategies. In reasoning and ethical safety, open models come close to matching closed models, despite using far fewer computational resources.</p> 
             <p>Overall, closed models maintain a lead in safety and inclusivity, while open models demonstrate that competitive and well-grounded results can be achieved efficiently without proprietary infrastructure.</p>
          </div>

          <!-- <div class="content has-text-centered">
            <div class="content has-text-centered">
              <img src="static/images/HumaniBenchOverviewScores.jpg" style="max-width:80%">
              <p class="content has-text-justified"><b> <span>Figure</span></b>:  HumaniBench principle-aligned scores. Each entry is the mean score of the tasks mapped to that principle (↑ higher is better). †Closed-source; all others open source.</p>
            </div>

          </div> -->

          <h3 class="title is-4 has-text-justified">Performance Across Social Attributes</h3>

          <div class="content has-text-centered">
            <p style="text-align: justify;">
              Recent evaluations show that <strong>closed-source models</strong> like GPT-4o and Gemini-2.0 consistently outperform open-source models across social attributes such as <em>age</em>, <em>race</em>, and <em>gender</em>, delivering more balanced and reliable results. While <strong>open-source models</strong> like CogVLM2 and Qwen2.5 perform well in specific areas like <em>race</em> and <em>sports</em>, they show greater variability in handling <em>gender</em> and <em>occupation</em>, especially across complex tasks like scene understanding and visual grounding.
            </p>

          </div>

          <!-- <h2 class="title is-3">Empirical and Qualitative Results</h2> -->

          <!-- <h3 class="title is-4 has-text-justified">1. Balancing Accuracy, Fairness, and Human-Centric Principles</h3> -->
          <div class="content has-text-centered">
            <div class="content has-text-centered">
              <img src="static/images/HumaniBenchT1-T3.jpg" style="max-width:80%">
              <p class="content has-text-justified"><b> <span>Figure</span></b>: Examples from T1 (Scene Understanding), T2 (Instance Identity), and T3 (Multiple-Choice VQA) with questions, ground-truth answers, and GPT-4o reasoning.</p>
            </div>


            <!-- <p style="text-align: justify;">
              While many open-source models show a trade-off between accuracy and fairness, top closed-source models (GPT-4o, Gemini-2.0) and Phi-4 demonstrate that careful data curation and fine-tuning can balance both. However, no model leads across faithfulness, contextual relevance, and coherence at the same time, underlining the need for multi-objective optimization to align competing human-centric goals.</p> -->
          </div>

          <!-- <h3 class="title is-4 has-text-justified">2. Race and Gender Remains the Most Challenging</h3> -->
          <!-- <div class="content has-text-centered">
            <p style="text-align: justify;">
              Across tasks, models show the highest bias and lowest accuracy on race and gender attributes, making these demographics the most challenging for fair and accurate analysis. Age and sports attributes perform more evenly, while occupation also shows notable bias, pointing to the need for targeted mitigation in sensitive areas.</p>
          </div> -->

          <!-- <br /> -->
          <!-- <h3 class="title is-4 has-text-justified">3. Multilingual Gaps Persist Across Models</h3> -->
          <div class="content has-text-centered">
            <div class="content has-text-centered">
              <img src="static/images/HumaniBenchMultilingual.jpg" style="max-width:80%">
              <p class="content has-text-justified"><b> <span>Figure</span></b>: Multilingual qualitative examples llustrating model performance across languages (French, Urdu, Tamil) and associated social attributes (Gender, Occupation, Age). Each column presents a question, ground truth answer, predicted answer, and error analysis.</p>
            </div>

            <p style="text-align: justify;">
              <!-- HumaniBench conducts multilingual evaluations across both open-ended and closed-ended tasks to assess disparities in accuracy and bias. By comparing performance on a mix of high-resource and low-resource languages, we observe that while high-resource languages show more consistent results, persistent gaps remain in low-resource language support. These findings underscore the ongoing challenges in achieving fair and equitable multilingual alignment.</p> -->
              <table style="width:60%; margin: 0 auto; border-collapse: collapse; font-family: sans-serif;">
                <thead style="background-color: #f2f2f2;">
                  <tr>
                    <th style="padding: 10px; border-bottom: 2px solid #ccc;">High-Resource Languages</th>
                    <th style="padding: 10px; border-bottom: 2px solid #ccc;">Low-Resource Languages</th>
                  </tr>
                </thead>
                <tbody>
                  <tr>
                    <td style="padding: 10px; border-bottom: 1px solid #e0e0e0;">English (Reference)</td>
                    <td style="padding: 10px; border-bottom: 1px solid #e0e0e0;">Urdu</td>
                  </tr>
                  <tr>
                    <td style="padding: 10px; border-bottom: 1px solid #e0e0e0;">French</td>
                    <td style="padding: 10px; border-bottom: 1px solid #e0e0e0;">Persian</td>
                  </tr>
                  <tr>
                    <td style="padding: 10px; border-bottom: 1px solid #e0e0e0;">Spanish</td>
                    <td style="padding: 10px; border-bottom: 1px solid #e0e0e0;">Bengali</td>
                  </tr>
                  <tr>
                    <td style="padding: 10px; border-bottom: 1px solid #e0e0e0;">Portuguese</td>
                    <td style="padding: 10px; border-bottom: 1px solid #e0e0e0;">Punjabi</td>
                  </tr>
                  <tr>
                    <td style="padding: 10px;">Mandarin</td>
                    <td style="padding: 10px;">Tamil</td>
                  </tr>
                  <tr>
                    <td style="padding: 10px;">Korean</td>
                  </tr>
                </tbody>
              </table>
              <p class="content has-text-justified"> <b> <span>Table</span></b>: Categorization of evaluation languages in the HumaniBench multilingual task. Languages are grouped into high-resource (e.g., English, French, Mandarin) and low-resource (e.g., Urdu, Tamil, Bengali) categories to assess model performance and fairness across varying linguistic resource availability. </p>
          </div>

          <!-- <h3 class="title is-4 has-text-justified">4. Weakly Supervised Localization Remains Challenging for LMMs</h3>
          <div class="content has-text-centered">
            <p style="text-align: justify;">
              Visual grounding tasks remain difficult, with some open-source models (Qwen-7B, LLaVA-v1.6) surprisingly outperforming closed-source systems like GPT-4o in precise localization. This suggests that open models currently hold an edge in certain specialized visual tasks.</p>
          </div> -->

          <!-- <h3 class="title is-4 has-text-justified">5. Closed-Source LMMs Exhibit Higher Empathy in Generated Captions</h3> -->

          <div class="content has-text-centered">
            <div class="content has-text-centered">
              <img src="static/images/HumaniBenchEmpathy.jpg" style="max-width:80%">
              <p class="content has-text-justified"><b> <span>Figure</span></b>: T6: Empathy & Human-Centric Response. Simple vs. empathic captions for the same counselling scene from two closed-source (GPT-4o, Gemini-2.0) and two open-source (Aya Vision, Phi-4) LMMs. LMMs. Linguistic tones—<span style="color:blue;">Analytic</span>, <span style="color:red;">Negative</span>, <span style="color:green;">Positive</span>—show empathic prompts lift Positive tone, add slight Negative wording, and keep Analytic steady, indicating prompt framing drives affective style in different models.</p>
            </div>


            <!-- <p style="text-align: justify;">
              Closed models like GPT-4o and Gemini 2.0 consistently produce genuinely empathetic captions that acknowledge emotional nuance and offer supportive interpretations. In contrast, most open-source models, such as Aya Vision and Phi-4, tend to default to literal or surface-level descriptions, revealing a clear empathy-tone gap. These differences highlight the strengths of closed models in human-centric alignment and the ongoing challenge of improving affective understanding in open models. The above figure also illustrates how models handle empathy in image captioning, comparing simple (analytic) and empathic (emotionally aware) styles.</p> -->
          </div>

          

          <!-- <h3 class="title is-4 has-text-justified">6. Robustness is Limited Under Real-World Perturbations</h3>
          <div class="content has-text-centered">
            <p style="text-align: justify;">
              No model fully withstands real-world perturbations, though Gemini-2.0 and some robustness-tuned open models like InternVL and CogVLM show relatively better resilience. Many models, however, experience steep performance drops under such conditions, underscoring ongoing robustness challenges.
            </p>
            

          </div>

          <h3 class="title is-4 has-text-justified">7. Step-by-step Prompting Using CoT Improves Performance</h3>
          <div class="content has-text-centered">
            <p style="text-align: justify;">
              Applying step-by-step reasoning through Chain-of-Thought (CoT) consistently boosts performance across models, with notable gains seen especially in Aya Vision and LLaVA-v1.6. This highlights the effectiveness of structured prompting strategies for enhancing model outputs.
            </p>

          </div>

          <h3 class="title is-4 has-text-justified">8. Scaling LMMs results in higher task accuracy</h3>
          <div class="content has-text-centered">
            <p style="text-align: justify;">
              Scaling up model size improves scene understanding performance across LMM families. Larger variants of GPT-4o, LLaVA-v1.6, Qwen2.5-VL, and LLaMA-3.2 consistently outperform smaller ones, with gains of 5–11%, highlighting the benefits of enhanced visual-textual alignment and broader knowledge capacity.
            </p>

          </div> -->

        </div>
      </div>

      <!-- <div class="content has-text-justified"> -->
        <!-- Center align  -->
        <!-- <div class="column is-full-width has-text-centered">
          <h2 class="title is-3">Social Impact</h2>
        </div> -->
        <!-- <h2 class="title is-4 has-text-centered">Conclusion</h2> -->
        <!-- <div class="content has-text-justified">
          <p>
            HumaniBench highlights both the potential and the societal risks of LMMs, revealing persistent issues like stereotyping, marginalization of low-resource languages, emotional misinterpretation, and hallucinations that could amplify bias or misinformation in critical domains. It also shows that safety filters can obscure important content, reducing transparency. Given the sensitive nature of its real-world data, HumaniBench use requires strong governance, bias audits, and human oversight to drive progress toward inclusive, trustworthy AI rather than endorsing partial solutions.</p>
        </div> -->
      <!-- </div> -->

      <div class="content has-text-justified">
        <!-- Center align  -->
        <div class="column is-full-width has-text-centered">
          <h2 class="title is-3">Conclusion</h2>
        </div>
        <!-- <h2 class="title is-4 has-text-centered">Conclusion</h2> -->
        <div class="content has-text-justified">
          <p>
            In conclusion, HumaniBench reveals that state-of-the-art LMMs still face significant trade-offs between accuracy and human-aligned behavior. While closed-source models generally perform better overall, they exhibit weaknesses in visual grounding. Open-source models show promise in specific areas but lack consistency. Techniques like chain-of-thought and scaling up models offer modest gains, yet alignment deficits persist. These insights underscore the need for multi-objective optimization—combining curated data, safety filters, and targeted fine-tuning—to advance truly human-aligned multimodal systems.</p>
          <br>
          <p>For additional details about HumaniBench evaluation and experimental results, please refer to our main paper. Thank you! </p>
        </div>
      </div>
    </div>



      

  </section>

  <section class="section" id="BibTeX">
    <div class="container is-max-desktop content">
      <h2 class="title"><a id="bibtex">BibTeX</a></h2>
      <pre><code>@misc{raza2025humanibenchhumancentricframeworklarge,
        title={HumaniBench: A Human-Centric Framework for Large Multimodal Models Evaluation}, 
        author={Shaina Raza and Aravind Narayanan and Vahid Reza Khazaie and Ashmal Vayani and Mukund S. Chettiar and Amandeep Singh and Mubarak Shah and Deval Pandya},
        year={2025},
        eprint={2505.11454},
        archivePrefix={arXiv},
        primaryClass={cs.CV},
        url={https://arxiv.org/abs/2505.11454}, 
  }</code></pre>
    </div>
  </section>


  <footer class="footer">
    <div class="columns is-centered">
      <div class="column is-8">
        <div class="content">
          <p class="has-text-centered">
            Website adapted from the following <a href="https://mingukkang.github.io/GigaGAN/">source code</a>.
          </p>
        </div>
      </div>
    </div>
    </div>
  </footer>


  <script src="juxtapose/js/juxtapose.js"></script>

  <script>
    var slider;
    let origOptions = {
      "makeResponsive": true,
      "showLabels": true,
      "mode": "horizontal",
      "showCredits": true,
      "animate": true,
      "startingPosition": "50"
    };

    const juxtaposeSelector = "#juxtapose-embed";
    const transientSelector = "#juxtapose-hidden";

    inputImage.src = "./static/images/".concat(name, "_input.jpg")
    outputImage.src = "./static/images/".concat(name, "_output.jpg")

    let images = [inputImage, outputImage];
    let options = slider.options;
    options.callback = function (obj) {
      var newNode = document.getElementById(obj.selector.substring(1));
      var oldNode = document.getElementById(juxtaposeSelector.substring(1));
      console.log(obj.selector.substring(1));
      console.log(newNode.children[0]);
      oldNode.replaceChild(newNode.children[0], oldNode.children[0]);
      //newNode.removeChild(newNode.children[0]);

    };

    slider = new juxtapose.JXSlider(transientSelector, images, options);
};



    (function () {
      slider = new juxtapose.JXSlider(
        juxtaposeSelector, origImages, origOptions);
      //document.getElementById("left-button").onclick = replaceLeft;
      //document.getElementById("right-button").onclick = replaceRight;
    })();
    // Get the image text
    var imgText = document.getElementById("imgtext");
    // Use the same src in the expanded image as the image being clicked on from the grid
    // expandImg.src = imgs.src;
    // Use the value of the alt attribute of the clickable image as text inside the expanded image
    imgText.innerHTML = name;
    // Show the container element (hidden with CSS)
    // expandImg.parentElement.style.display = "block";

    $(".flip-card").click(function () {
      console.log("fading in")
      div_back = $(this).children().children()[1]
      div_front = $(this).children().children()[0]
      // console.log($(this).children("div.flip-card-back"))
      console.log(div_back)
      $(div_front).addClass("out");
      $(div_front).removeClass("in");

      $(div_back).addClass("in");
      $(div_back).removeClass("out");

    });

    $(".flip-card").mouseleave(function () {
      console.log("fading in")
      div_back = $(this).children().children()[1]
      div_front = $(this).children().children()[0]
      // console.log($(this).children("div.flip-card-back"))
      console.log(div_back)
      $(div_front).addClass("in");
      $(div_front).removeClass("out");

      $(div_back).addClass("out");
      $(div_back).removeClass("in");

    });

  </script>
  <!-- <script src="https://ajax.googleapis.com/ajax/libs/jquery/1.7.1/jquery.min.js" type="text/javascript"></script> -->
  <script src="https://cdn.jsdelivr.net/npm/popper.js@1.12.9/dist/umd/popper.min.js"
    integrity="sha384-ApNbgh9B+Y1QKtv3Rn7W3mgPxhU9K/ScQsAP7hUibX39j7fakFPskvXusvfa0b4Q"
    crossorigin="anonymous"></script>
  <script src="https://cdn.jsdelivr.net/npm/bootstrap@3.3.7/dist/js/bootstrap.min.js"></script>

</body>

</html>
